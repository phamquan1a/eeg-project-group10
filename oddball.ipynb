{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13988040,"sourceType":"datasetVersion","datasetId":8915892}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install hmmlearn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:19:59.427323Z","iopub.execute_input":"2025-12-05T20:19:59.427670Z","iopub.status.idle":"2025-12-05T20:20:04.109149Z","shell.execute_reply.started":"2025-12-05T20:19:59.427637Z","shell.execute_reply":"2025-12-05T20:20:04.108440Z"}},"outputs":[{"name":"stdout","text":"Collecting hmmlearn\n  Downloading hmmlearn-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.26.4)\nRequirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.2.2)\nRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.15.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->hmmlearn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->hmmlearn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->hmmlearn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->hmmlearn) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->hmmlearn) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.10->hmmlearn) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10->hmmlearn) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10->hmmlearn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.10->hmmlearn) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.10->hmmlearn) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.10->hmmlearn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.10->hmmlearn) (2024.2.0)\nDownloading hmmlearn-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.9/165.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: hmmlearn\nSuccessfully installed hmmlearn-0.3.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================\n# CELL 1 — Import & Config\n# ============================\n\nimport os\nfrom glob import glob\nimport numpy as np\nimport mne\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.decomposition import PCA\nfrom hmmlearn import hmm\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n# Config\nBASE_PATH = \"/kaggle/input/eeg-oddball/EEG\"\nTMIN, TMAX = -0.2, 0.8\nFMIN, FMAX = 0.1, 20\nP300_WINDOW = (0.25, 0.35)\nRANDOM_STATE = 42\nBATCH_SIZE = 64\nEPOCHS = 30\nLEARNING_RATE = 1e-3\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device:\", DEVICE)\n\n# List subjects\nsubject_dirs = sorted(glob(os.path.join(BASE_PATH, \"sub*\")))\nprint(\"Found subjects:\", len(subject_dirs))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:20:04.110571Z","iopub.execute_input":"2025-12-05T20:20:04.110799Z","iopub.status.idle":"2025-12-05T20:20:10.882293Z","shell.execute_reply.started":"2025-12-05T20:20:04.110776Z","shell.execute_reply":"2025-12-05T20:20:10.881571Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nFound subjects: 42\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================\n# CELL 2 — Process one subject\n# ============================\n\ndef process_subject_folder(sub_folder, event_id, tmin=TMIN, tmax=TMAX,\n                           fmin=FMIN, fmax=FMAX):\n\n    vhdr_files = glob(os.path.join(sub_folder, \"*.vhdr\"))\n    if len(vhdr_files) == 0:\n        raise FileNotFoundError(f\"No .vhdr found in {sub_folder}\")\n\n    vhdr = vhdr_files[0]\n    raw = mne.io.read_raw_brainvision(vhdr, preload=True, verbose=False)\n\n    raw.filter(fmin, fmax, fir_design='firwin', verbose=False)\n    raw.pick_types(meg=False, eeg=True, eog=True, stim=False, verbose=False)\n\n    events, event_dict = mne.events_from_annotations(raw, verbose=False)\n\n    epochs = mne.Epochs(\n        raw, events, event_id=event_id, tmin=tmin, tmax=tmax,\n        baseline=(None, 0), preload=True, verbose=False\n    )\n\n    if len(epochs) == 0:\n        print(f\"Warning: no epochs for {sub_folder}\")\n        return None, None, None, None, None\n\n    X = epochs.get_data()\n    y = epochs.events[:, -1]\n    return X, y, epochs.times, raw.info[\"sfreq\"], event_dict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:20:10.883037Z","iopub.execute_input":"2025-12-05T20:20:10.883450Z","iopub.status.idle":"2025-12-05T20:20:10.889854Z","shell.execute_reply.started":"2025-12-05T20:20:10.883423Z","shell.execute_reply":"2025-12-05T20:20:10.889146Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============================\n# CELL 3 — Extract event_dict\n# ============================\n\n# Load 1 subject to get event_ids\ntmp_sub = subject_dirs[0]\ntmp_vhdr = glob(os.path.join(tmp_sub, \"*.vhdr\"))[0]\ntmp_raw = mne.io.read_raw_brainvision(tmp_vhdr, preload=False, verbose=False)\ntmp_events, tmp_dict = mne.events_from_annotations(tmp_raw, verbose=False)\n\nprint(\"Event_dict:\", tmp_dict)\n\nEVENT_ID = {\n    \"standard\": tmp_dict[\"Stimulus/S  6\"],\n    \"oddball\":  tmp_dict[\"Stimulus/S  7\"]\n}\nprint(\"Using EVENT_ID:\", EVENT_ID)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:20:10.891281Z","iopub.execute_input":"2025-12-05T20:20:10.891513Z","iopub.status.idle":"2025-12-05T20:20:12.951929Z","shell.execute_reply.started":"2025-12-05T20:20:10.891496Z","shell.execute_reply":"2025-12-05T20:20:12.951302Z"}},"outputs":[{"name":"stdout","text":"Event_dict: {'New Segment/': 99999, 'Stimulus/S  1': 1, 'Stimulus/S  5': 5, 'Stimulus/S  6': 6, 'Stimulus/S  7': 7, 'Stimulus/S 10': 10, 'Stimulus/S 12': 12, 'Time 0/': 10001}\nUsing EVENT_ID: {'standard': 6, 'oddball': 7}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================\n# CELL 4 — Load all subjects\n# ============================\n\nall_X = []\nall_y = []\nall_sub_idx = []\n\nfor i, sub in enumerate(subject_dirs):\n    print(f\"Processing {sub} ({i+1}/{len(subject_dirs)})\")\n\n    X, y, times, sfreq, evdict = process_subject_folder(sub, EVENT_ID)\n    if X is None:\n        continue\n\n    all_X.append(X)\n    all_y.append(y)\n    all_sub_idx.append(np.full(len(y), i))\n\nX_all = np.vstack(all_X)\ny_all = np.hstack(all_y)\nsub_idx_all = np.hstack(all_sub_idx)\n\nprint(\"Total trials:\", X_all.shape)\nprint(\"Label distribution:\", np.unique(y_all, return_counts=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:20:12.952721Z","iopub.execute_input":"2025-12-05T20:20:12.953190Z","iopub.status.idle":"2025-12-05T20:32:26.292537Z","shell.execute_reply.started":"2025-12-05T20:20:12.953140Z","shell.execute_reply":"2025-12-05T20:32:26.291624Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/eeg-oddball/EEG/sub 01 (1/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 02 (2/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 03 (3/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 04 (4/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 05 (5/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 06 (6/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 07 (7/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 08 (8/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 09 (9/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 10 (10/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 11 (11/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 12 (12/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 13 (13/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 14 (14/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 15 (15/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 16 (16/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 17 (17/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 18 (18/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 19 (19/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 20 (20/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 21 (21/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 22 (22/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 23 (23/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 24 (24/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 25 (25/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 26 (26/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 27 (27/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 28 (28/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 29 (29/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 30 (30/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 31 (31/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 32 (32/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 33 (33/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 34 (34/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 35 (35/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 36 (36/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 37 (37/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 38 (38/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 39 (39/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 40 (40/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 41 (41/42)\nProcessing /kaggle/input/eeg-oddball/EEG/sub 42 (42/42)\nTotal trials: (6847, 127, 1001)\nLabel distribution: (array([6, 7]), array([3422, 3425]))\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================\n# CELL 5 — Feature Extraction\n# ============================\n\n# Convert labels to 0/1\nlabel_map = {\n    list(EVENT_ID.values())[0]: 0,\n    list(EVENT_ID.values())[1]: 1\n}\ny_bin = np.array([label_map[v] for v in y_all])\n\n# P300 ERP mean amplitude\nt = times\nmask = (t >= P300_WINDOW[0]) & (t <= P300_WINDOW[1])\n\ndef extract_erp_features(X):\n    seg_mean = X[:, :, mask].mean(axis=2)\n    return seg_mean.reshape(seg_mean.shape[0], -1)\n\nX_feat = extract_erp_features(X_all)\nprint(\"ERP feature shape:\", X_feat.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:32:26.293435Z","iopub.execute_input":"2025-12-05T20:32:26.293675Z","iopub.status.idle":"2025-12-05T20:32:28.116977Z","shell.execute_reply.started":"2025-12-05T20:32:26.293648Z","shell.execute_reply":"2025-12-05T20:32:28.116227Z"}},"outputs":[{"name":"stdout","text":"ERP feature shape: (6847, 127)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================\n# CELL 5b — HMM Feature Extraction\n# ============================\n\nN_STATES = 4      # số hidden states\nN_PCA = 5         # số chiều PCA cho HMM\n\nprint(\"\\n=== Extracting HMM features ===\")\n\n# ---- Step 1: PCA across each trial (flatten ch × time) ----\nX_flat = X_all.reshape(X_all.shape[0], -1)\npca = PCA(n_components=N_PCA)\nX_pca = pca.fit_transform(X_flat)\n\n# reshape lại thành chuỗi time series cho HMM\nX_seq = X_pca.reshape(X_all.shape[0], -1, 1)  # shape: (trials, time', 1)\n\n# ---- Step 2: Train 1 global HMM for all data ----\nX_concat = np.vstack([x for x in X_seq])    # concatenate into (T_total, 1)\nlengths = [len(x) for x in X_seq]           # sequence lengths\n\nhmm_model = hmm.GaussianHMM(\n    n_components=N_STATES,\n    covariance_type=\"diag\",\n    n_iter=100,\n    random_state=RANDOM_STATE\n)\n\nprint(\"Training HMM...\")\nhmm_model.fit(X_concat, lengths)\n\n# ---- Step 3: Extract HMM features for each trial ----\ndef extract_hmm_features(model, X_seq):\n    feats = []\n    for seq in X_seq:\n        ll = model.score(seq)  # log-likelihood\n        states = model.predict(seq)\n        # state fraction\n        frac = np.bincount(states, minlength=model.n_components) / len(states)\n        # transition prob flatten\n        trans = model.transmat_.flatten()\n        f = np.concatenate([[ll], frac, trans])\n        feats.append(f)\n    return np.array(feats)\n\nX_hmm = extract_hmm_features(hmm_model, X_seq)\nprint(\"HMM feature shape:\", X_hmm.shape)\n\n# ---- Step 4: Combine ERP + HMM features ----\nX_feat_full = np.concatenate([X_feat, X_hmm], axis=1)\nprint(\"Final feature shape:\", X_feat_full.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:32:28.117921Z","iopub.execute_input":"2025-12-05T20:32:28.118385Z","iopub.status.idle":"2025-12-05T20:33:39.062130Z","shell.execute_reply.started":"2025-12-05T20:32:28.118336Z","shell.execute_reply":"2025-12-05T20:33:39.061538Z"}},"outputs":[{"name":"stdout","text":"\n=== Extracting HMM features ===\nTraining HMM...\n","output_type":"stream"},{"name":"stderr","text":"Model is not converging.  Current: 168607.20383680574 is not greater than 168618.2039171142. Delta is -11.000080308469478\n","output_type":"stream"},{"name":"stdout","text":"HMM feature shape: (6847, 21)\nFinal feature shape: (6847, 148)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================\n# CELL 6 — Train ML models\n# ============================\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_feat, y_bin, test_size=0.2, random_state=RANDOM_STATE, stratify=y_bin\n)\n\nscaler = StandardScaler().fit(X_train)\nX_train_s = scaler.transform(X_train)\nX_test_s = scaler.transform(X_test)\n\n# Logistic Regression\nprint(\"\\n--- Logistic Regression ---\")\nclf_lr = LogisticRegression(max_iter=1000)\nclf_lr.fit(X_train_s, y_train)\npred_lr = clf_lr.predict(X_test_s)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_lr))\nprint(classification_report(y_test, pred_lr))\n\n# Random Forest\nprint(\"\\n--- Random Forest ---\")\nclf_rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE)\nclf_rf.fit(X_train_s, y_train)\npred_rf = clf_rf.predict(X_test_s)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_rf))\nprint(classification_report(y_test, pred_rf))\n\n# SVM RBF\nprint(\"\\n--- SVM (RBF Kernel) ---\")\nclf_svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")\nclf_svm.fit(X_train_s, y_train)\npred_svm = clf_svm.predict(X_test_s)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_svm))\nprint(classification_report(y_test, pred_svm))\n\n\n# Gradient Boosting\nprint(\"\\n--- Gradient Boosting ---\")\nclf_gb = GradientBoostingClassifier()\nclf_gb.fit(X_train, y_train)  # Tree-based models không cần scale\npred_gb = clf_gb.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_gb))\nprint(classification_report(y_test, pred_gb))\n\n\n# XGBoost\nprint(\"\\n--- XGBoost ---\")\ntry:\n    clf_xgb = XGBClassifier(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=5,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        objective=\"binary:logistic\",\n        eval_metric=\"logloss\"\n    )\n    clf_xgb.fit(X_train, y_train)\n    pred_xgb = clf_xgb.predict(X_test)\n    print(\"Accuracy:\", accuracy_score(y_test, pred_xgb))\n    print(classification_report(y_test, pred_xgb))\nexcept:\n    print(\"⚠️ XGBoost không khả dụng trên môi trường này.\")\n\n\n# LDA\nprint(\"\\n--- LDA (Linear Discriminant Analysis) ---\")\nclf_lda = LinearDiscriminantAnalysis()\nclf_lda.fit(X_train_s, y_train)\npred_lda = clf_lda.predict(X_test_s)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_lda))\nprint(classification_report(y_test, pred_lda))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:33:39.062870Z","iopub.execute_input":"2025-12-05T20:33:39.063215Z","iopub.status.idle":"2025-12-05T20:34:19.177754Z","shell.execute_reply.started":"2025-12-05T20:33:39.063186Z","shell.execute_reply":"2025-12-05T20:34:19.176146Z"}},"outputs":[{"name":"stdout","text":"\n--- Logistic Regression ---\nAccuracy: 0.5328467153284672\n              precision    recall  f1-score   support\n\n           0       0.53      0.50      0.52       685\n           1       0.53      0.56      0.55       685\n\n    accuracy                           0.53      1370\n   macro avg       0.53      0.53      0.53      1370\nweighted avg       0.53      0.53      0.53      1370\n\n\n--- Random Forest ---\nAccuracy: 0.5211678832116788\n              precision    recall  f1-score   support\n\n           0       0.52      0.55      0.53       685\n           1       0.52      0.49      0.51       685\n\n    accuracy                           0.52      1370\n   macro avg       0.52      0.52      0.52      1370\nweighted avg       0.52      0.52      0.52      1370\n\n\n--- SVM (RBF Kernel) ---\nAccuracy: 0.5401459854014599\n              precision    recall  f1-score   support\n\n           0       0.54      0.54      0.54       685\n           1       0.54      0.54      0.54       685\n\n    accuracy                           0.54      1370\n   macro avg       0.54      0.54      0.54      1370\nweighted avg       0.54      0.54      0.54      1370\n\n\n--- Gradient Boosting ---\nAccuracy: 0.518978102189781\n              precision    recall  f1-score   support\n\n           0       0.51      0.76      0.61       685\n           1       0.54      0.28      0.36       685\n\n    accuracy                           0.52      1370\n   macro avg       0.52      0.52      0.49      1370\nweighted avg       0.52      0.52      0.49      1370\n\n\n--- XGBoost ---\nAccuracy: 0.5364963503649635\n              precision    recall  f1-score   support\n\n           0       0.54      0.53      0.53       685\n           1       0.54      0.54      0.54       685\n\n    accuracy                           0.54      1370\n   macro avg       0.54      0.54      0.54      1370\nweighted avg       0.54      0.54      0.54      1370\n\n\n--- LDA (Linear Discriminant Analysis) ---\nAccuracy: 0.5262773722627737\n              precision    recall  f1-score   support\n\n           0       0.53      0.50      0.51       685\n           1       0.52      0.55      0.54       685\n\n    accuracy                           0.53      1370\n   macro avg       0.53      0.53      0.53      1370\nweighted avg       0.53      0.53      0.53      1370\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================\n# CELL 6b — Train ML models with HMM\n# ============================\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_feat_full, y_bin, test_size=0.2, random_state=RANDOM_STATE, stratify=y_bin\n)\n\n\nscaler = StandardScaler().fit(X_train)\nX_train_s = scaler.transform(X_train)\nX_test_s = scaler.transform(X_test)\n\n# Logistic Regression\nprint(\"\\n--- Logistic Regression ---\")\nclf_lr = LogisticRegression(max_iter=1000)\nclf_lr.fit(X_train_s, y_train)\npred_lr = clf_lr.predict(X_test_s)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_lr))\nprint(classification_report(y_test, pred_lr))\n\n# Random Forest\nprint(\"\\n--- Random Forest ---\")\nclf_rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE)\nclf_rf.fit(X_train_s, y_train)\npred_rf = clf_rf.predict(X_test_s)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_rf))\nprint(classification_report(y_test, pred_rf))\n\n# SVM RBF\nprint(\"\\n--- SVM (RBF Kernel) ---\")\nclf_svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")\nclf_svm.fit(X_train_s, y_train)\npred_svm = clf_svm.predict(X_test_s)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_svm))\nprint(classification_report(y_test, pred_svm))\n\n\n# Gradient Boosting\nprint(\"\\n--- Gradient Boosting ---\")\nclf_gb = GradientBoostingClassifier()\nclf_gb.fit(X_train, y_train)  # Tree-based models không cần scale\npred_gb = clf_gb.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_gb))\nprint(classification_report(y_test, pred_gb))\n\n\n# XGBoost\nprint(\"\\n--- XGBoost ---\")\ntry:\n    clf_xgb = XGBClassifier(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=5,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        objective=\"binary:logistic\",\n        eval_metric=\"logloss\"\n    )\n    clf_xgb.fit(X_train, y_train)\n    pred_xgb = clf_xgb.predict(X_test)\n    print(\"Accuracy:\", accuracy_score(y_test, pred_xgb))\n    print(classification_report(y_test, pred_xgb))\nexcept:\n    print(\"⚠️ XGBoost không khả dụng trên môi trường này.\")\n\n\n# LDA\nprint(\"\\n--- LDA (Linear Discriminant Analysis) ---\")\nclf_lda = LinearDiscriminantAnalysis()\nclf_lda.fit(X_train_s, y_train)\npred_lda = clf_lda.predict(X_test_s)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_lda))\nprint(classification_report(y_test, pred_lda))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:34:19.178461Z","iopub.execute_input":"2025-12-05T20:34:19.178693Z","iopub.status.idle":"2025-12-05T20:34:58.787911Z","shell.execute_reply.started":"2025-12-05T20:34:19.178673Z","shell.execute_reply":"2025-12-05T20:34:58.786525Z"}},"outputs":[{"name":"stdout","text":"\n--- Logistic Regression ---\nAccuracy: 0.5357664233576642\n              precision    recall  f1-score   support\n\n           0       0.54      0.53      0.53       685\n           1       0.54      0.54      0.54       685\n\n    accuracy                           0.54      1370\n   macro avg       0.54      0.54      0.54      1370\nweighted avg       0.54      0.54      0.54      1370\n\n\n--- Random Forest ---\nAccuracy: 0.5401459854014599\n              precision    recall  f1-score   support\n\n           0       0.54      0.54      0.54       685\n           1       0.54      0.54      0.54       685\n\n    accuracy                           0.54      1370\n   macro avg       0.54      0.54      0.54      1370\nweighted avg       0.54      0.54      0.54      1370\n\n\n--- SVM (RBF Kernel) ---\nAccuracy: 0.5467153284671533\n              precision    recall  f1-score   support\n\n           0       0.55      0.56      0.55       685\n           1       0.55      0.53      0.54       685\n\n    accuracy                           0.55      1370\n   macro avg       0.55      0.55      0.55      1370\nweighted avg       0.55      0.55      0.55      1370\n\n\n--- Gradient Boosting ---\nAccuracy: 0.5138686131386861\n              precision    recall  f1-score   support\n\n           0       0.51      0.62      0.56       685\n           1       0.52      0.41      0.46       685\n\n    accuracy                           0.51      1370\n   macro avg       0.51      0.51      0.51      1370\nweighted avg       0.51      0.51      0.51      1370\n\n\n--- XGBoost ---\nAccuracy: 0.5321167883211679\n              precision    recall  f1-score   support\n\n           0       0.53      0.51      0.52       685\n           1       0.53      0.55      0.54       685\n\n    accuracy                           0.53      1370\n   macro avg       0.53      0.53      0.53      1370\nweighted avg       0.53      0.53      0.53      1370\n\n\n--- LDA (Linear Discriminant Analysis) ---\nAccuracy: 0.5401459854014599\n              precision    recall  f1-score   support\n\n           0       0.54      0.54      0.54       685\n           1       0.54      0.54      0.54       685\n\n    accuracy                           0.54      1370\n   macro avg       0.54      0.54      0.54      1370\nweighted avg       0.54      0.54      0.54      1370\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================\n# CELL 7 — Prepare DL inputs\n# ============================\n\nX_dl = X_all.astype(np.float32)\nN = X_dl.shape[0]\n\n# Same split as ML\nidx = np.arange(N)\nidx_train, idx_test = train_test_split(idx, test_size=0.2,\n                                       random_state=RANDOM_STATE,\n                                       stratify=y_bin)\n\nch_mean = X_dl[idx_train].mean(axis=(0,2))\nch_std = X_dl[idx_train].std(axis=(0,2)) + 1e-9\n\nfor c in range(X_dl.shape[1]):\n    X_dl[:, c, :] = (X_dl[:, c, :] - ch_mean[c]) / ch_std[c]\n\nX_dl = X_dl[:, np.newaxis, :, :]\n\nclass EEGDataset(Dataset):\n    def __init__(self, X, y, indices):\n        self.X = torch.from_numpy(X[indices])\n        self.y = torch.from_numpy(y[indices].astype(np.int64))\n    def __len__(self):\n        return len(self.y)\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntrain_dataset = EEGDataset(X_dl, y_bin, idx_train)\ntest_dataset = EEGDataset(X_dl, y_bin, idx_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:34:58.791201Z","iopub.execute_input":"2025-12-05T20:34:58.791418Z","iopub.status.idle":"2025-12-05T20:35:10.434823Z","shell.execute_reply.started":"2025-12-05T20:34:58.791400Z","shell.execute_reply":"2025-12-05T20:35:10.434191Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ============================\n# CELL 8 — EEGNet model\n# ============================\n\nclass SimpleEEGNet(nn.Module):\n    def __init__(self, in_ch, n_times, n_classes=2, F1=8, D=2):\n        super().__init__()\n        self.temp_conv = nn.Conv2d(1, F1, kernel_size=(1, 64),\n                                   padding=(0,32), bias=False)\n        self.bn1 = nn.BatchNorm2d(F1)\n\n        self.depthwise = nn.Conv2d(F1, F1*D, kernel_size=(in_ch,1),\n                                   groups=F1, bias=False)\n        self.bn2 = nn.BatchNorm2d(F1*D)\n\n        self.pool = nn.AvgPool2d((1,4))\n        self.drop = nn.Dropout(0.5)\n\n        out_time = n_times // 4\n        self.classify = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(F1*D * out_time, 64),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(64, n_classes)\n        )\n\n    def forward(self, x):\n        x = torch.relu(self.bn1(self.temp_conv(x)))\n        x = torch.relu(self.bn2(self.depthwise(x)))\n        x = self.pool(x)\n        x = self.drop(x)\n        return self.classify(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:35:10.435973Z","iopub.execute_input":"2025-12-05T20:35:10.436377Z","iopub.status.idle":"2025-12-05T20:35:10.445621Z","shell.execute_reply.started":"2025-12-05T20:35:10.436352Z","shell.execute_reply":"2025-12-05T20:35:10.444667Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ============================\n# CELL 8b — ShallowConvNet model\n# ============================\nclass ShallowConvNet(nn.Module):\n    def __init__(self, in_ch, n_times, n_classes=2):\n        super().__init__()\n\n        # Temporal convolution\n        self.conv_time = nn.Conv2d(\n            1, 40, kernel_size=(1, 25), stride=1, padding=(0, 12), bias=False\n        )\n        self.bn_time = nn.BatchNorm2d(40)\n\n        # Spatial convolution (depthwise)\n        self.conv_spat = nn.Conv2d(\n            40, 40, kernel_size=(in_ch, 1), groups=40, bias=False\n        )\n        self.bn_spat = nn.BatchNorm2d(40)\n\n        # Mean Pooling with big kernel (the original paper uses 75)\n        self.pool = nn.AvgPool2d(kernel_size=(1, 75), stride=(1, 15))\n\n        # Compute output size dynamically\n        dummy = torch.zeros(1, 1, in_ch, n_times)\n        out = self._forward_features(dummy)\n        self.flatten_dim = out.numel()\n\n        self.classifier = nn.Linear(self.flatten_dim, n_classes)\n\n    def _forward_features(self, x):\n        x = self.conv_time(x)\n        x = self.bn_time(x)\n\n        x = self.conv_spat(x)\n        x = self.bn_spat(x)\n\n        x = x ** 2                         # Square activation\n        x = self.pool(x)\n        x = torch.log(torch.clamp(x, min=1e-6))  # Log activation\n        return x\n\n    def forward(self, x):\n        x = self._forward_features(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:35:10.446712Z","iopub.execute_input":"2025-12-05T20:35:10.447087Z","iopub.status.idle":"2025-12-05T20:35:10.466833Z","shell.execute_reply.started":"2025-12-05T20:35:10.447064Z","shell.execute_reply":"2025-12-05T20:35:10.465793Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ============================\n# CELL 8c — DeepConvNet model\n# ============================\nclass DeepConvNet(nn.Module):\n    def __init__(self, in_ch, n_times, n_classes=2, dropout=0.5):\n        super().__init__()\n\n        self.block1 = nn.Sequential(\n            nn.Conv2d(1, 25, kernel_size=(1, 5), stride=1, padding=(0, 2), bias=False),\n            nn.Conv2d(25, 25, kernel_size=(in_ch, 1), groups=25, bias=False),\n            nn.BatchNorm2d(25),\n            nn.ELU(),\n            nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2)),\n            nn.Dropout(dropout),\n        )\n\n        self.block2 = nn.Sequential(\n            nn.Conv2d(25, 50, kernel_size=(1, 5), padding=(0, 2), bias=False),\n            nn.BatchNorm2d(50),\n            nn.ELU(),\n            nn.MaxPool2d((1, 2)),\n            nn.Dropout(dropout),\n        )\n\n        self.block3 = nn.Sequential(\n            nn.Conv2d(50, 100, kernel_size=(1, 5), padding=(0, 2), bias=False),\n            nn.BatchNorm2d(100),\n            nn.ELU(),\n            nn.MaxPool2d((1, 2)),\n            nn.Dropout(dropout),\n        )\n\n        self.block4 = nn.Sequential(\n            nn.Conv2d(100, 200, kernel_size=(1, 5), padding=(0, 2), bias=False),\n            nn.BatchNorm2d(200),\n            nn.ELU(),\n            nn.MaxPool2d((1, 2)),\n            nn.Dropout(dropout),\n        )\n\n        # Compute feature dimensions\n        dummy = torch.zeros(1, 1, in_ch, n_times)\n        out = self._forward_features(dummy)\n        self.flatten_dim = out.numel()\n\n        self.classifier = nn.Linear(self.flatten_dim, n_classes)\n\n    def _forward_features(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        return x\n\n    def forward(self, x):\n        x = self._forward_features(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:35:10.468249Z","iopub.execute_input":"2025-12-05T20:35:10.468515Z","iopub.status.idle":"2025-12-05T20:35:10.486894Z","shell.execute_reply.started":"2025-12-05T20:35:10.468489Z","shell.execute_reply":"2025-12-05T20:35:10.486200Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ============================\n# CELL 9 — Train DL model\n# ============================\nn_channels = X_dl.shape[2]\nn_times = X_dl.shape[3]\nmodels = {\n    \"EEGNet\": SimpleEEGNet(n_channels, n_times).to(DEVICE),\n    \"ShallowConvNet\": ShallowConvNet(n_channels, n_times).to(DEVICE),\n    \"DeepConvNet\": DeepConvNet(n_channels, n_times).to(DEVICE),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:36:13.909089Z","iopub.execute_input":"2025-12-05T20:36:13.909863Z","iopub.status.idle":"2025-12-05T20:36:14.444466Z","shell.execute_reply.started":"2025-12-05T20:36:13.909838Z","shell.execute_reply":"2025-12-05T20:36:14.443562Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ============================\n# CELL 9 — Train DL model\n# ============================\ndef train_one_model(model, name, train_loader, test_loader, epochs=EPOCHS, lr=LEARNING_RATE):\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    best_acc = 0\n    best_path = f\"best_{name}.pth\"\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n\n        for xb, yb in train_loader:\n            xb = xb.to(DEVICE)\n            yb = yb.to(DEVICE)\n\n            optimizer.zero_grad()\n            out = model(xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item() * xb.size(0)\n\n        # ===== Evaluation =====\n        model.eval()\n        preds, labels = [], []\n\n        with torch.no_grad():\n            for xb, yb in test_loader:\n                xb = xb.to(DEVICE)\n                out = model(xb)\n                p = out.argmax(1).cpu().numpy()\n                preds.append(p)\n                labels.append(yb.numpy())\n\n        preds = np.concatenate(preds)\n        labels = np.concatenate(labels)\n        acc = (preds == labels).mean()\n\n        print(f\"[{name}] Epoch {epoch+1}/{epochs}  Loss={total_loss/len(train_dataset):.4f}  Acc={acc:.4f}\")\n\n        if acc > best_acc:\n            best_acc = acc\n            torch.save(model.state_dict(), best_path)\n\n    print(f\"Best accuracy for {name}: {best_acc:.4f}\\n\")\n    return best_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:36:22.014446Z","iopub.execute_input":"2025-12-05T20:36:22.014959Z","iopub.status.idle":"2025-12-05T20:36:22.022344Z","shell.execute_reply.started":"2025-12-05T20:36:22.014934Z","shell.execute_reply":"2025-12-05T20:36:22.021666Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# ============================\n# CELL 9 — Train DL model\n# ============================\nresults = {}\n\nfor name, model in models.items():\n    print(\"========================================\")\n    print(f\"Training model: {name}\")\n    print(\"========================================\")\n    acc = train_one_model(model, name, train_loader, test_loader,\n                          epochs=EPOCHS, lr=LEARNING_RATE)\n    results[name] = acc\n\nprint(\"====== FINAL RESULTS ======\")\nfor k, v in results.items():\n    print(f\"{k}: {v:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T20:36:28.336511Z","iopub.execute_input":"2025-12-05T20:36:28.337226Z","iopub.status.idle":"2025-12-05T21:16:55.432855Z","shell.execute_reply.started":"2025-12-05T20:36:28.337196Z","shell.execute_reply":"2025-12-05T21:16:55.432070Z"}},"outputs":[{"name":"stdout","text":"========================================\nTraining model: EEGNet\n========================================\n[EEGNet] Epoch 1/30  Loss=0.6917  Acc=0.6000\n[EEGNet] Epoch 2/30  Loss=0.6511  Acc=0.6336\n[EEGNet] Epoch 3/30  Loss=0.6269  Acc=0.6285\n[EEGNet] Epoch 4/30  Loss=0.6021  Acc=0.6679\n[EEGNet] Epoch 5/30  Loss=0.5879  Acc=0.6796\n[EEGNet] Epoch 6/30  Loss=0.5641  Acc=0.6985\n[EEGNet] Epoch 7/30  Loss=0.5555  Acc=0.6898\n[EEGNet] Epoch 8/30  Loss=0.5369  Acc=0.6898\n[EEGNet] Epoch 9/30  Loss=0.5267  Acc=0.6964\n[EEGNet] Epoch 10/30  Loss=0.5086  Acc=0.7117\n[EEGNet] Epoch 11/30  Loss=0.5065  Acc=0.7146\n[EEGNet] Epoch 12/30  Loss=0.5011  Acc=0.7095\n[EEGNet] Epoch 13/30  Loss=0.4798  Acc=0.7073\n[EEGNet] Epoch 14/30  Loss=0.4791  Acc=0.7073\n[EEGNet] Epoch 15/30  Loss=0.4689  Acc=0.7036\n[EEGNet] Epoch 16/30  Loss=0.4674  Acc=0.7058\n[EEGNet] Epoch 17/30  Loss=0.4613  Acc=0.7153\n[EEGNet] Epoch 18/30  Loss=0.4452  Acc=0.7241\n[EEGNet] Epoch 19/30  Loss=0.4390  Acc=0.7182\n[EEGNet] Epoch 20/30  Loss=0.4405  Acc=0.7153\n[EEGNet] Epoch 21/30  Loss=0.4286  Acc=0.7263\n[EEGNet] Epoch 22/30  Loss=0.4196  Acc=0.7175\n[EEGNet] Epoch 23/30  Loss=0.4246  Acc=0.7124\n[EEGNet] Epoch 24/30  Loss=0.4055  Acc=0.7146\n[EEGNet] Epoch 25/30  Loss=0.3929  Acc=0.7153\n[EEGNet] Epoch 26/30  Loss=0.3933  Acc=0.7168\n[EEGNet] Epoch 27/30  Loss=0.3838  Acc=0.7095\n[EEGNet] Epoch 28/30  Loss=0.3797  Acc=0.7299\n[EEGNet] Epoch 29/30  Loss=0.3980  Acc=0.7226\n[EEGNet] Epoch 30/30  Loss=0.3820  Acc=0.7095\nBest accuracy for EEGNet: 0.7299\n\n========================================\nTraining model: ShallowConvNet\n========================================\n[ShallowConvNet] Epoch 1/30  Loss=0.8688  Acc=0.5934\n[ShallowConvNet] Epoch 2/30  Loss=0.6655  Acc=0.5978\n[ShallowConvNet] Epoch 3/30  Loss=0.6238  Acc=0.5964\n[ShallowConvNet] Epoch 4/30  Loss=0.6551  Acc=0.6117\n[ShallowConvNet] Epoch 5/30  Loss=0.5771  Acc=0.6314\n[ShallowConvNet] Epoch 6/30  Loss=0.5481  Acc=0.6285\n[ShallowConvNet] Epoch 7/30  Loss=0.5445  Acc=0.6467\n[ShallowConvNet] Epoch 8/30  Loss=0.5264  Acc=0.6693\n[ShallowConvNet] Epoch 9/30  Loss=0.5447  Acc=0.6416\n[ShallowConvNet] Epoch 10/30  Loss=0.4781  Acc=0.6453\n[ShallowConvNet] Epoch 11/30  Loss=0.4745  Acc=0.6599\n[ShallowConvNet] Epoch 12/30  Loss=0.4424  Acc=0.6547\n[ShallowConvNet] Epoch 13/30  Loss=0.4363  Acc=0.6394\n[ShallowConvNet] Epoch 14/30  Loss=0.4406  Acc=0.6380\n[ShallowConvNet] Epoch 15/30  Loss=0.4175  Acc=0.6394\n[ShallowConvNet] Epoch 16/30  Loss=0.4031  Acc=0.6642\n[ShallowConvNet] Epoch 17/30  Loss=0.3713  Acc=0.6255\n[ShallowConvNet] Epoch 18/30  Loss=0.3819  Acc=0.6226\n[ShallowConvNet] Epoch 19/30  Loss=0.3891  Acc=0.6504\n[ShallowConvNet] Epoch 20/30  Loss=0.3337  Acc=0.6577\n[ShallowConvNet] Epoch 21/30  Loss=0.3329  Acc=0.6474\n[ShallowConvNet] Epoch 22/30  Loss=0.3263  Acc=0.6562\n[ShallowConvNet] Epoch 23/30  Loss=0.3022  Acc=0.6620\n[ShallowConvNet] Epoch 24/30  Loss=0.2924  Acc=0.6438\n[ShallowConvNet] Epoch 25/30  Loss=0.2678  Acc=0.6372\n[ShallowConvNet] Epoch 26/30  Loss=0.2822  Acc=0.6504\n[ShallowConvNet] Epoch 27/30  Loss=0.2900  Acc=0.6372\n[ShallowConvNet] Epoch 28/30  Loss=0.2559  Acc=0.6620\n[ShallowConvNet] Epoch 29/30  Loss=0.2748  Acc=0.6358\n[ShallowConvNet] Epoch 30/30  Loss=0.2401  Acc=0.6584\nBest accuracy for ShallowConvNet: 0.6693\n\n========================================\nTraining model: DeepConvNet\n========================================\n[DeepConvNet] Epoch 1/30  Loss=1.0713  Acc=0.5934\n[DeepConvNet] Epoch 2/30  Loss=0.8491  Acc=0.5942\n[DeepConvNet] Epoch 3/30  Loss=0.8143  Acc=0.5453\n[DeepConvNet] Epoch 4/30  Loss=0.7531  Acc=0.6321\n[DeepConvNet] Epoch 5/30  Loss=0.7600  Acc=0.6620\n[DeepConvNet] Epoch 6/30  Loss=0.7075  Acc=0.6584\n[DeepConvNet] Epoch 7/30  Loss=0.7258  Acc=0.6606\n[DeepConvNet] Epoch 8/30  Loss=0.7227  Acc=0.6343\n[DeepConvNet] Epoch 9/30  Loss=0.6913  Acc=0.6708\n[DeepConvNet] Epoch 10/30  Loss=0.6766  Acc=0.6788\n[DeepConvNet] Epoch 11/30  Loss=0.6699  Acc=0.6124\n[DeepConvNet] Epoch 12/30  Loss=0.6419  Acc=0.6540\n[DeepConvNet] Epoch 13/30  Loss=0.6545  Acc=0.6613\n[DeepConvNet] Epoch 14/30  Loss=0.6765  Acc=0.6788\n[DeepConvNet] Epoch 15/30  Loss=0.6643  Acc=0.6905\n[DeepConvNet] Epoch 16/30  Loss=0.6352  Acc=0.6701\n[DeepConvNet] Epoch 17/30  Loss=0.6215  Acc=0.5861\n[DeepConvNet] Epoch 18/30  Loss=0.6399  Acc=0.6839\n[DeepConvNet] Epoch 19/30  Loss=0.6083  Acc=0.6599\n[DeepConvNet] Epoch 20/30  Loss=0.6376  Acc=0.6861\n[DeepConvNet] Epoch 21/30  Loss=0.6063  Acc=0.6518\n[DeepConvNet] Epoch 22/30  Loss=0.5963  Acc=0.6971\n[DeepConvNet] Epoch 23/30  Loss=0.5914  Acc=0.6657\n[DeepConvNet] Epoch 24/30  Loss=0.5798  Acc=0.6891\n[DeepConvNet] Epoch 25/30  Loss=0.5900  Acc=0.6752\n[DeepConvNet] Epoch 26/30  Loss=0.5981  Acc=0.6723\n[DeepConvNet] Epoch 27/30  Loss=0.5701  Acc=0.6971\n[DeepConvNet] Epoch 28/30  Loss=0.5694  Acc=0.6985\n[DeepConvNet] Epoch 29/30  Loss=0.5650  Acc=0.6964\n[DeepConvNet] Epoch 30/30  Loss=0.5777  Acc=0.6759\nBest accuracy for DeepConvNet: 0.6985\n\n====== FINAL RESULTS ======\nEEGNet: 0.7299\nShallowConvNet: 0.6693\nDeepConvNet: 0.6985\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ============================\n# CELL 10 — Evaluate + Save\n# ============================\ndef evaluate_model(model, checkpoint_path, test_loader, device):\n    # Load lại trọng số của model\n    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    model.eval()\n\n    preds, labels = [], []\n\n    with torch.no_grad():\n        for xb, yb in test_loader:\n            xb = xb.to(device)\n            out = model(xb)\n            p = out.argmax(1).cpu().numpy()\n\n            preds.append(p)\n            labels.append(yb.numpy())\n\n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n\n    acc = accuracy_score(labels, preds)\n    print(\"Final Accuracy:\", acc)\n    print(classification_report(labels, preds))\n    print(confusion_matrix(labels, preds))\n\n    return acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T21:16:55.434304Z","iopub.execute_input":"2025-12-05T21:16:55.434808Z","iopub.status.idle":"2025-12-05T21:16:55.440773Z","shell.execute_reply.started":"2025-12-05T21:16:55.434786Z","shell.execute_reply":"2025-12-05T21:16:55.439984Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# ============================\n# CELL 10 — Evaluate + Save\n# ============================\n\nmodel_paths = {\n    \"EEGNet\": \"best_EEGNet.pth\",\n    \"Shallow\": \"best_ShallowConvNet.pth\",\n    \"DeepConvNet\": \"best_DeepConvNet.pth\"\n}\n\nfor name, path in model_paths.items():\n    print(\"\\n===== Evaluating\", name, \"=====\")\n\n    # tạo model tương ứng\n    if name == \"EEGNet\":\n        model = SimpleEEGNet(n_channels, n_times).to(DEVICE)\n    elif name == \"Shallow\":\n        model = ShallowConvNet(n_channels, n_times).to(DEVICE)\n    elif name == \"DeepConvNet\":\n        model = DeepConvNet(n_channels, n_times).to(DEVICE)\n\n    evaluate_model(model, path, test_loader, DEVICE)\n\nimport joblib\njoblib.dump(clf_lr, \"logreg.pkl\")\njoblib.dump(clf_rf, \"rf.pkl\")\njoblib.dump(clf_svm,\"svm.pkl\")\njoblib.dump(clf_gb,\"gb.pkl\")\njoblib.dump(clf_xgb,\"xgb.pkl\")\njoblib.dump(clf_lda,\"lda.pkl\")\njoblib.dump(scaler, \"scaler.pkl\")\n\nprint(\"Saved: logreg.pkl, rf.pkl, svm.pkl, gb.pkl, xgb.pkl, lda.pkl, best_EEGNet.pth, best_ShallowConvNet.pth, best_DeepConvNet.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T21:17:57.454024Z","iopub.execute_input":"2025-12-05T21:17:57.454618Z","iopub.status.idle":"2025-12-05T21:18:00.145836Z","shell.execute_reply.started":"2025-12-05T21:17:57.454590Z","shell.execute_reply":"2025-12-05T21:18:00.144997Z"}},"outputs":[{"name":"stdout","text":"\n===== Evaluating EEGNet =====\nFinal Accuracy: 0.7299270072992701\n              precision    recall  f1-score   support\n\n           0       0.72      0.76      0.74       685\n           1       0.74      0.70      0.72       685\n\n    accuracy                           0.73      1370\n   macro avg       0.73      0.73      0.73      1370\nweighted avg       0.73      0.73      0.73      1370\n\n[[519 166]\n [204 481]]\n\n===== Evaluating Shallow =====\nFinal Accuracy: 0.6693430656934306\n              precision    recall  f1-score   support\n\n           0       0.64      0.79      0.70       685\n           1       0.72      0.55      0.63       685\n\n    accuracy                           0.67      1370\n   macro avg       0.68      0.67      0.66      1370\nweighted avg       0.68      0.67      0.66      1370\n\n[[539 146]\n [307 378]]\n\n===== Evaluating DeepConvNet =====\nFinal Accuracy: 0.6985401459854015\n              precision    recall  f1-score   support\n\n           0       0.67      0.79      0.72       685\n           1       0.74      0.61      0.67       685\n\n    accuracy                           0.70      1370\n   macro avg       0.71      0.70      0.70      1370\nweighted avg       0.71      0.70      0.70      1370\n\n[[542 143]\n [270 415]]\nSaved: logreg.pkl, rf.pkl, svm.pkl, gb.pkl, xgb.pkl, lda.pkl, best_EEGNet.pth, best_ShallowConvNet.pth, best_DeepConvNet.pth\n","output_type":"stream"}],"execution_count":21}]}